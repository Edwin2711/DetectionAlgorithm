{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import pathlib\n",
        "\n",
        "from PIL import Image\n",
        "from io import open\n",
        "import torch.functional as F\n",
        "from torchvision.models import squeezenet1_1\n",
        "import time"
      ],
      "metadata": {
        "id": "Dll_q-GKIaIc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIRST STEP:** Detect if it's a propane gas cylinder or not. Difference bigger than 25% would stop the code."
      ],
      "metadata": {
        "id": "IIyNAoHbJOwF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af-ouwhiDuZT",
        "outputId": "e279593e-55a6-4424-8554-ab16996bd22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image matching Error between the two images: 40.07815538194444\n"
          ]
        }
      ],
      "source": [
        "# load the input images\n",
        "img1 = cv2.imread('/content/prueba21.jpg')\n",
        "img2 = cv2.imread('/content/galonGasolina.jpg')\n",
        "\n",
        "img1 = cv2.resize(img1,(480,480))\n",
        "img2 = cv2.resize(img2,(480,480))\n",
        "\n",
        "# convert the images to grayscale\n",
        "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# define the function to compute MSE between two images\n",
        "def mse(img1, img2):\n",
        "   h, w = img1.shape\n",
        "   diff = cv2.subtract(img1, img2)\n",
        "   err = np.sum(diff**2)\n",
        "   mse = err/(float(h*w))\n",
        "   return mse, diff\n",
        "\n",
        "error, diff = mse(img1, img2)\n",
        "print(\"Image matching Error between the two images:\",error)\n",
        "\n",
        "#cv2_imshow(diff)\n",
        "position = (5,30)\n",
        "text = \"Difference = \" + str(int(error)) +\"%\"\n",
        "cv2.putText(\n",
        "     diff, #numpy array on which text is written\n",
        "     text, #text\n",
        "     position, #position at which writing has to start\n",
        "     cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
        "     1, #font size\n",
        "     (209, 80, 0, 255), #font color\n",
        "     3) #font stroke\n",
        "cv2.imwrite('output.png', diff)\n",
        "\n",
        "cv2_imshow(cv2.imread('output.png'))\n",
        "if error > 25:\n",
        "    print(\"No cylinder detected\")\n",
        "    os._exit(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECOND STEP:** Classifying if it is a big or small cylinder."
      ],
      "metadata": {
        "id": "TudYlXY1K64k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('/content/prueba21.jpg')\n",
        "\n",
        "mask = np.zeros(img.shape[:2],np.uint8)\n",
        "\n",
        "bgdModel = np.zeros((1,65),np.float64)\n",
        "fgdModel = np.zeros((1,65),np.float64)\n",
        "\n",
        "rect = (129,24,338,443)\n",
        "\n",
        "#applying grabcut\n",
        "cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
        "mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
        "img = img*mask2[:,:,np.newaxis]\n",
        "\n",
        "#thresholding\n",
        "cvimg = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "ret,thresh = cv2.threshold(cvimg,0,255,cv2.THRESH_BINARY)\n",
        "\n",
        "count = cv2.countNonZero(thresh)\n",
        "\n",
        "#print(\"White \" , count)\n",
        "\n",
        "resized_image = cv2.resize(thresh, (480,480))\n",
        "#cv2_imshow(resized_image)\n",
        "#plt.imshow(img)\n",
        "#plt.colorbar()\n",
        "#plt.show()\n",
        "(height, width, _) = img.shape\n",
        "percentage =( count/(height*width) )*100\n",
        "#print(\"percentage\",percentage)\n",
        "\n",
        "position = (5,30)\n",
        "text = \"Occupied area percentage = \" + str(int(percentage)) +\"%\"\n",
        "cv2.putText(\n",
        "     resized_image, #numpy array on which text is written\n",
        "     text, #text\n",
        "     position, #position at which writing has to start\n",
        "     cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
        "     0.8, #font size\n",
        "     (209, 80, 0, 255), #font color\n",
        "     3) #font stroke\n",
        "cv2.imwrite('output.png', resized_image)\n",
        "\n",
        "cv2_imshow(cv2.imread('output.png'))\n",
        "\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "if percentage > 50:\n",
        "  print(\"It's a big cylinder\")\n",
        "else:\n",
        "  print(\"It's a small cylinder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "MMWgiyzWK5b5",
        "outputId": "1e259db3-02aa-47e5-b270-e6fcbf51525a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=480x480 at 0x7F3DB1CF1630>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's a small cylinder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THIRD STEP:** Recognizing if it's a composite or steel cylinder."
      ],
      "metadata": {
        "id": "4g7B2VjGMlSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN Network\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,num_classes=2):\n",
        "        super(ConvNet,self).__init__()\n",
        "        \n",
        "        #Output size after convolution filter\n",
        "        #((w-f+2P)/s) +1\n",
        "        \n",
        "        #Input shape= (32,3,224,224)\n",
        "        \n",
        "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
        "        #Shape= (32,12,224,224)\n",
        "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
        "        self.relu1=nn.ReLU()\n",
        "\n",
        "        \n",
        "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
        "        #Reduce the image size be factor 2\n",
        "        #Shape= (32,12,112,112)\n",
        "        \n",
        "        \n",
        "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
        "        #Shape= (32,20,112,112)\n",
        "        self.bn2=nn.BatchNorm2d(num_features=20)\n",
        "        self.relu2=nn.ReLU()\n",
        "        \n",
        "\n",
        "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
        "        #Shape= (32,32,112,112)\n",
        "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
        "        self.relu3=nn.ReLU()\n",
        "        \n",
        "        \n",
        "        self.fc=nn.Linear(in_features= 112 * 112 * 32,out_features=num_classes)\n",
        "\n",
        "        #Feed forwad function\n",
        "        \n",
        "    def forward(self,input):\n",
        "        output=self.conv1(input)\n",
        "        output=self.bn1(output)\n",
        "        output=self.relu1(output)\n",
        "            \n",
        "        output=self.pool(output)\n",
        "            \n",
        "        output=self.conv2(output)\n",
        "        output=self.bn2(output)\n",
        "        output=self.relu2(output)\n",
        "            \n",
        "        output=self.conv3(output)\n",
        "        output=self.bn3(output)\n",
        "        output=self.relu3(output)\n",
        "\n",
        "        output=output.view(-1,32*112*112)\n",
        "            \n",
        "            \n",
        "        output=self.fc(output)   \n",
        "        return output"
      ],
      "metadata": {
        "id": "oGSelMVNTPjl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint=torch.load('best_checkpoint.model',map_location=torch.device('cpu'))\n",
        "model=ConvNet(num_classes=2)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()\n",
        "\n",
        "#Transforms\n",
        "transformer=transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
        "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
        "                        [0.5,0.5,0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "CluwbgNLN3U_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction function\n",
        "def prediction(img_path,transformer):\n",
        "    \n",
        "    classes = ['composite', 'steel']\n",
        "    image=Image.open(img_path)\n",
        "    \n",
        "    image_tensor=transformer(image).float()\n",
        "    \n",
        "    \n",
        "    image_tensor=image_tensor.unsqueeze_(0)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        image_tensor.cuda()\n",
        "        \n",
        "    input=Variable(image_tensor)\n",
        "    \n",
        "    \n",
        "    output=model(input)\n",
        "    index=output.data.numpy().argmax()\n",
        "    pred=classes[index]\n",
        "    return pred\n",
        "    "
      ],
      "metadata": {
        "id": "Dzjue_XPSx4-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('prueba21.jpg',transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QPTM3SDTUw95",
        "outputId": "870316d5-c0cf-4c5d-92ea-e21f71d8bde1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'composite'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}